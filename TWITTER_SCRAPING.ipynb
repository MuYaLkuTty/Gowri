{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrokNDMLL6IExJtphYFkVj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuYaLkuTty/Gowri/blob/main/TWITTER_SCRAPING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project 01:TWITTER SCRAPING"
      ],
      "metadata": {
        "id": "KSiFda0zqVgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skills take away From This Project\n",
        "1.Python scripting\n",
        "2.Data Collection\n",
        "3.MongoDB \n",
        "4.Streamlit\n",
        "5.Domain\n",
        "6.Social Media\n"
      ],
      "metadata": {
        "id": "C83aP7BYqgbJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9Rx93T1qOs8",
        "outputId": "7e42d3cd-7d03-484d-837d-374589b31fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting snscrape\n",
            "  Downloading snscrape-0.6.2.20230320-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from snscrape) (3.10.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from snscrape) (4.11.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.9/dist-packages (from snscrape) (2.27.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from snscrape) (4.9.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->snscrape) (2.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape) (2.0.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Installing collected packages: snscrape\n",
            "Successfully installed snscrape-0.6.2.20230320\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.1/492.1 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython<3.0.0,>=1.16.0\n",
            "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 KB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.3.0 pymongo-4.3.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.20.0-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (5.3.0)\n",
            "Collecting blinker>=1.0.0\n",
            "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: altair<5,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (4.5.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from streamlit) (2.8.2)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchdog\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from streamlit) (1.22.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (8.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.9/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: pandas<2,>=0.25 in /usr/local/lib/python3.9/dist-packages (from streamlit) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (8.4.0)\n",
            "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.9/dist-packages (from streamlit) (6.2)\n",
            "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.9/dist-packages (from streamlit) (23.0)\n",
            "Collecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.4 in /usr/local/lib/python3.9/dist-packages (from streamlit) (2.27.1)\n",
            "Collecting rich>=10.11.0\n",
            "  Downloading rich-13.3.2-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.7/238.7 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.9/dist-packages (from streamlit) (3.19.6)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.9/dist-packages (from streamlit) (4.2)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 KB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (3.1.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=1.4->streamlit) (3.15.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas<2,>=0.25->streamlit) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil->streamlit) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (1.26.15)\n",
            "Collecting markdown-it-py<3.0.0,>=2.2.0\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments<3.0.0,>=2.13.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.9/dist-packages (from tzlocal>=1.1->streamlit) (0.1.0.post0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from validators>=0.2->streamlit) (4.4.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->altair<5,>=3.2.0->streamlit) (2.1.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit) (22.2.0)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.9/dist-packages (from pytz-deprecation-shim->tzlocal>=1.1->streamlit) (2022.7)\n",
            "Building wheels for collected packages: validators\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19579 sha256=6c37fc30a20ac62a78f67275316b3dbc8b2be583f4fea3125aa89e181c15d00b\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/f0/a8/1094fca7a7e5d0d12ff56e0c64675d72aa5cc81a5fc200e849\n",
            "Successfully built validators\n",
            "Installing collected packages: watchdog, validators, smmap, semver, pympler, pygments, mdurl, blinker, pydeck, markdown-it-py, gitdb, rich, gitpython, streamlit\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blinker-1.5 gitdb-4.0.10 gitpython-3.1.31 markdown-it-py-2.2.0 mdurl-0.1.2 pydeck-0.8.0 pygments-2.14.0 pympler-1.0.1 rich-13.3.2 semver-2.13.0 smmap-5.0.0 streamlit-1.20.0 validators-0.20.0 watchdog-3.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.2.1.tar.gz (761 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.2.1-py3-none-any.whl size=19790 sha256=1b2496fb4c5cb4af720af6f0754a9086c3675afd6d4ed7368aebd220478e4210\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/89/59/49d4249e00957e94813ac136a335d10ed2e09a856c5096f95c\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.2.1\n"
          ]
        }
      ],
      "source": [
        "#Installing packages\n",
        "!pip install snscrape\n",
        "!pip install pymongo\n",
        "!pip install pandas\n",
        "!pip install streamlit\n",
        "!pip install pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary modules are imported\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "from PIL import Image\n",
        "from datetime import date\n",
        "import json\n",
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "BrNMALYAr63X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  %%writefile twitterscraping.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1paaxMTvhUo",
        "outputId": "1e389170-8bfe-4fa7-c112-4a761441ad5a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing twitterscraping.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MongoDB client connection is done\n",
        "client = pymongo.MongoClient(\"mongodb+srv://gowrishankar:201197@cluster0.yaryscs.mongodb.net/?retryWrites=true&w=majority\")\n",
        "twtdb = client.gowrishankar\n",
        "twtdb_main = twtdb.twitterproj\n",
        "\n",
        "# Here starts the main function\n",
        "def main():\n",
        "  tweets = 0\n",
        "  st.title(\"Twitter Scraping\")\n",
        "  # Menus used in Twitter Scrape web app -- 5 menus are used\n",
        "  menu = [\"Home\",\"About\",\"Search\",\"Display\",\"Download\"]\n",
        "  choice = st.sidebar.selectbox(\"Menu\",menu)\n",
        "\n",
        "  \n",
        "  ngrok.set_auth_token(\"2NJrgqRqePOGB1PbQN7WgfzOpKP_7z8TiVbNVoxGRW9xKjBdX\")\n",
        "\n",
        "  # Menu 1 is Home page \n",
        "  \n",
        "  if choice==\"Home\":\n",
        "    st.write('''This app is a Twitter Scraping web app created using Streamlit. \n",
        "             It scrapes the twitter data for the given hashtag/ keyword for the given period.\n",
        "             The tweets are uploaded in MongoDB and can be dowloaded as CSV or a JSON file.''')\n",
        "\n",
        "  # Menu 2 is about the Twitter Scrape libraries, databases and apps\n",
        "  elif choice==\"About\":\n",
        "    # Info about Twitter Scrapper\n",
        "    with st.expander(\"Twitter Scrapper\"):\n",
        "      st.write('''Twitter Scraper will scrape the data from Public Twitter profiles.\n",
        "                    It will collect the data about **date, id, url, tweet content, users/tweeters,reply count, \n",
        "                    retweet count, language, source, like count, followers, friends** and lot more information \n",
        "                    to gather the real facts about the Tweets.''')\n",
        "\n",
        "    # Info about Snscraper\n",
        "    with st.expander(\"Snscraper\"):\n",
        "      st.write('''Snscrape is a scraper for social media services like *twitter, faceboook, instagram and so on*. \n",
        "                   It scrapes **user profiles, hashtages, other tweet information** and returns the discovered items from the relavent posts/tweets.''')\n",
        "\n",
        "    # Info about MongoDB database\n",
        "    with st.expander(\"Mondodb\"):\n",
        "      st.write('''MongoDB is an open source document database used for storing unstrcutured data. The data is stored as JSON like documents called BSON. \n",
        "                  It is used by developers to work esaily with real time data analystics, content management and lot of other web applications.''')\n",
        "\n",
        "    # Info about Streamlit framework\n",
        "    with st.expander(\"Streamlit\"):\n",
        "      st.write('''Streamlit is a **awesome opensource framwork used for buidling highly interactive sharable web applications*** in python language. \n",
        "                  Its easy to share *machine learning and datasciecne web apps* using streamlit.\n",
        "                  It allows the app to load the large set of datas from web for manipulation and  performing expensive computations.''')\n",
        "      \n",
        "\n",
        "  # Menu 3 is a search option\n",
        "  elif choice==\"Search\":\n",
        "    # Every time after the last tweet the database will be cleared for updating new scraping data\n",
        "    twtdb_main.delete_many({})\n",
        "\n",
        "    # Form for collecting user input for twitter scrape\n",
        "    with st.form(key='form1'):\n",
        "      # Hashtag input\n",
        "      st.subheader(\"Tweet searching Form\")\n",
        "      st.write(\"Enter the hashtag or keyword to perform the twitter scraping👇#\")\n",
        "      query = st.text_input('Hashtag or keyword')\n",
        "\n",
        "      # No of tweets for scraping\n",
        "      st.write(\"Enter the limit for the data scraping: Maximum limit is 1000 tweets\")\n",
        "      limit = st.number_input('Insert a number',min_value=0,max_value=1000,step=10)\n",
        "\n",
        "      # From date to end date for scraping\n",
        "      st.write(\"Enter the Starting date to scrap the tweet data\")\n",
        "      start = st.date_input('Start date')\n",
        "      end = st.date_input('End date')\n",
        "      \n",
        "      # Submit button to scrap\n",
        "      submit_button = st.form_submit_button(label=\"Tweet Scrap\")\n",
        "    \n",
        "    if submit_button:\n",
        "      st.success(f\"Tweet hashtag {query} received for scraping\".format(query))\n",
        "\n",
        "      # TwitterSearchScraper will scrape the data and insert into MongoDB database\n",
        "      for tweet in sntwitter.TwitterSearchScraper(f'from:{query} since:{start} until:{end}').get_items():\n",
        "        # To verify the limit if condition is set\n",
        "        if tweets == limit:\n",
        "          break\n",
        "        # Stores the tweet data into MongoDB until the limit  is reached\n",
        "        else:      \n",
        "          new = {\"date\":tweet.date,\"user\":tweet.user.username, \"url\":tweet.url, \"followersCount\":tweet.user.followersCount, \"friendsCount\":tweet.user.friendsCount, \"favouritesCount\":tweet.user.favouritesCount,\"mediaCount\":tweet.user.mediaCount}\n",
        "          twtdb_main.insert_one(new)\n",
        "          tweets += 1\n",
        "      \n",
        "      # Display the total tweets scraped\n",
        "      df = pd.DataFrame(list(twtdb_main.find()))\n",
        "      cnt = len(df)\n",
        "      st.success(f\"Total number of tweets scraped for the input query is := {cnt}\".format(cnt))\n",
        "\n",
        "  # Menu 4 is for diaplying the data uploaded in MmongoDB\n",
        "  elif choice==\"Display\":\n",
        "    # Save the documents in a dataframe\n",
        "    df = pd.DataFrame(list(twtdb_main.find()))\n",
        "    #Dispaly the document \n",
        "    st.dataframe(df)  \n",
        "\n",
        "  # Menu 5 is for Downloading the scraped data as CSV or JSON\n",
        "  else:\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    # Download the scraped data as CSV\n",
        "    with col1:\n",
        "      st.write(\"Download the tweet data as CSV File\")\n",
        "      # save the documents in a dataframe\n",
        "      df = pd.DataFrame(list(twtdb_main.find()))\n",
        "      # Convert dataframe to csv\n",
        "      df.to_csv('twittercsv.csv')\n",
        "      def convert_df(data):\n",
        "        # Cache the conversion to prevent computation on every rerun\n",
        "        return df.to_csv().encode('utf-8')\n",
        "      csv = convert_df(df)\n",
        "      st.download_button(\n",
        "                        label=\"Download data as CSV\",\n",
        "                        data=csv,\n",
        "                        file_name='twtittercsv.csv',\n",
        "                        mime='text/csv',\n",
        "                        )\n",
        "      st.success(\"Successfully Downloaded data as CSV\")\n",
        "\n",
        "    # Download the scraped data as JSON\n",
        "    with col2:\n",
        "      st.write(\"Download the tweet data as JSON File\")\n",
        "      # Convert dataframe to json string instead as json file \n",
        "      twtjs = df.to_json(default_handler=str).encode()\n",
        "      # Create Python object from JSON string data\n",
        "      obj = json.loads(twtjs)\n",
        "      js = json.dumps(obj, indent=4)\n",
        "      st.download_button(\n",
        "                        label=\"Download data as JSON\",\n",
        "                        data=js,\n",
        "                        file_name='twtjs.js',\n",
        "                        mime='text/js',\n",
        "                        )\n",
        "      st.success(\"Successfully Downloaded data as JSON\")\n",
        "!nohup streamlit run twitterscraping.py --server.port 10 &\n",
        "url = ngrok.connect(port = '10')\n",
        "print(url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdTcSidUsgzI",
        "outputId": "f1701e7a-8b11-4020-b909-5193b7bdee4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.ngrok:Opening tunnel named: http-80-a23561c5-5285-49a2-8e48-76c11045cc31\n",
            "2023-03-21 16:34:09.921 INFO    pyngrok.ngrok: Opening tunnel named: http-80-a23561c5-5285-49a2-8e48-76c11045cc31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process.ngrok:t=2023-03-21T16:34:12+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "2023-03-21 16:34:12.856 INFO    pyngrok.process.ngrok: t=2023-03-21T16:34:12+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "INFO:pyngrok.process.ngrok:t=2023-03-21T16:34:12+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "2023-03-21 16:34:12.864 INFO    pyngrok.process.ngrok: t=2023-03-21T16:34:12+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.ngrok2/ngrok.yml\n",
            "INFO:pyngrok.process.ngrok:t=2023-03-21T16:34:12+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "2023-03-21 16:34:12.876 INFO    pyngrok.process.ngrok: t=2023-03-21T16:34:12+0000 lvl=info msg=\"open config file\" path=/root/.ngrok2/ngrok.yml err=nil\n",
            "INFO:pyngrok.process.ngrok:t=2023-03-21T16:34:12+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "2023-03-21 16:34:12.891 INFO    pyngrok.process.ngrok: t=2023-03-21T16:34:12+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040\n",
            "INFO:pyngrok.process.ngrok:t=2023-03-21T16:34:13+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "2023-03-21 16:34:13.033 INFO    pyngrok.process.ngrok: t=2023-03-21T16:34:13+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "INFO:pyngrok.process.ngrok:t=2023-03-21T16:34:13+0000 lvl=info msg=\"client session established\" obj=csess id=83691182a596\n",
            "2023-03-21 16:34:13.037 INFO    pyngrok.process.ngrok: t=2023-03-21T16:34:13+0000 lvl=info msg=\"client session established\" obj=csess id=83691182a596\n",
            "INFO:pyngrok.process.ngrok:t=2023-03-21T16:34:13+0000 lvl=info msg=start pg=/api/tunnels id=fa412dc4020cd07c\n",
            "2023-03-21 16:34:13.052 INFO    pyngrok.process.ngrok: t=2023-03-21T16:34:13+0000 lvl=info msg=start pg=/api/tunnels id=fa412dc4020cd07c\n",
            "INFO:pyngrok.process.ngrok:t=2023-03-21T16:34:13+0000 lvl=info msg=end pg=/api/tunnels id=fa412dc4020cd07c status=200 dur=2.327233ms\n",
            "2023-03-21 16:34:13.059 INFO    pyngrok.process.ngrok: t=2023-03-21T16:34:13+0000 lvl=info msg=end pg=/api/tunnels id=fa412dc4020cd07c status=200 dur=2.327233ms\n",
            "INFO:pyngrok.process.ngrok:t=2023-03-21T16:34:13+0000 lvl=info msg=start pg=/api/tunnels id=041c4d2cabe3a837\n",
            "2023-03-21 16:34:13.067 INFO    pyngrok.process.ngrok: t=2023-03-21T16:34:13+0000 lvl=info msg=start pg=/api/tunnels id=041c4d2cabe3a837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NgrokTunnel: \"http://ec28-34-74-110-54.ngrok.io\" -> \"http://localhost:80\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pyngrok.process.ngrok:t=2023-03-21T16:34:13+0000 lvl=info msg=end pg=/api/tunnels id=041c4d2cabe3a837 status=200 dur=139.032µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20sk99Sous-y",
        "outputId": "f162769a-522f-4b80-fca2-7e1a84cf1960"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:\n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n",
            "2023-03-21 16:34:18.833 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.9/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n",
            "INFO:pyngrok.process:Updating authtoken for default \"config_path\" of \"ngrok_path\": /usr/local/lib/python3.9/dist-packages/pyngrok/bin/ngrok\n",
            "2023-03-21 16:34:18.838 Updating authtoken for default \"config_path\" of \"ngrok_path\": /usr/local/lib/python3.9/dist-packages/pyngrok/bin/ngrok\n"
          ]
        }
      ]
    }
  ]
}